---
title: "Exploratory Data Analysis"
author: "J. Stratton"
date: "March 16, 2016"
output: html_document
---

```{r load_libs, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(SnowballC)
```

```{r exploring_txts, echo=FALSE, cache=TRUE}
# Load full files
docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

```

```{r analyze_raw_txt, echo=FALSE, cache=TRUE}

line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

chars_per_line <- lapply(X = texts, FUN = function(x){nchar(x[[1]])})

words_per_line <- lapply(X = texts, FUN = function(x){
        sapply(X = strsplit(x[[1]], split = "[[:space:]]+"), FUN = length)
})
```

```{r output_raw_counts, echo=FALSE}
line_count_output <- data.frame(line_count, row.names = c("Blogs", "News", "Twitter"))
colnames(line_count_output) <- "Number of Lines"
line_count_output

chars_per_line_output <- sapply(X = chars_per_line, FUN = summary)
words_per_line_output <- sapply(X = words_per_line, FUN = summary)

colnames(chars_per_line_output) <- c("Blogs", "News", "Twitter")
colnames(words_per_line_output) <- c("Blogs", "News", "Twitter")

chars_per_line_output
words_per_line_output
```

```{r create_clean_corpuses, echo=FALSE, cache=TRUE}
# Clean the text in the corpus

docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

# Remove whitespace
texts <- tm_map(texts, stripWhitespace)

# Remove the rarely used low info characters from the corpus
texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

# Remove swear words part II
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_complex_swear_words))

# Remove webaddresses
rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_web_addresses))

# Remove stop words
texts <- tm_map(texts, removeWords, stopwords("english"))

# Remove any whitespace my cleaning regexes created
texts <- tm_map(texts, stripWhitespace)
```

```{r create_random_samples, echo=FALSE, cache=TRUE}
line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

# Use a random 5% sample from the corpus for each sample
pcnt <- 5
set.seed(3202016)

# Make sample 1
sample_text_1 <- texts

sample_text_1[[1]][[1]] <- sample(x = sample_text_1[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))
sample_text_1[[2]][[1]] <- sample(x = sample_text_1[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))
sample_text_1[[3]][[1]] <- sample(x = sample_text_1[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 2
sample_text_2 <- texts

sample_text_2[[1]][[1]] <- sample(x = sample_text_2[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_2[[2]][[1]] <- sample(x = sample_text_2[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_2[[3]][[1]] <- sample(x = sample_text_2[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 3
sample_text_3 <- texts

sample_text_3[[1]][[1]] <- sample(x = sample_text_3[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_3[[2]][[1]] <- sample(x = sample_text_3[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_3[[3]][[1]] <- sample(x = sample_text_3[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 4
sample_text_4 <- texts

sample_text_4[[1]][[1]] <- sample(x = sample_text_4[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_4[[2]][[1]] <- sample(x = sample_text_4[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_4[[3]][[1]] <- sample(x = sample_text_4[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 5
sample_text_5 <- texts

sample_text_5[[1]][[1]] <- sample(x = sample_text_5[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_5[[2]][[1]] <- sample(x = sample_text_5[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_5[[3]][[1]] <- sample(x = sample_text_5[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

```

```{r analyze_rand_samples, echo=FALSE, cache=TRUE}
# Analyze 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Analyze 2- through 4-grams
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)


n_gram_freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
n_gram_freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
n_gram_freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
n_gram_freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
n_gram_freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Sort the frequency lists
freq_1 <- lapply(X = freq_1, FUN = sort, decreasing = TRUE)
freq_2 <- lapply(X = freq_2, FUN = sort, decreasing = TRUE)
freq_3 <- lapply(X = freq_3, FUN = sort, decreasing = TRUE)
freq_4 <- lapply(X = freq_4, FUN = sort, decreasing = TRUE)
freq_5 <- lapply(X = freq_5, FUN = sort, decreasing = TRUE)

n_gram_freq_1 <- lapply(X = n_gram_freq_1, FUN = sort, decreasing = TRUE)
n_gram_freq_2 <- lapply(X = n_gram_freq_2, FUN = sort, decreasing = TRUE)
n_gram_freq_3 <- lapply(X = n_gram_freq_3, FUN = sort, decreasing = TRUE)
n_gram_freq_4 <- lapply(X = n_gram_freq_4, FUN = sort, decreasing = TRUE)
n_gram_freq_5 <- lapply(X = n_gram_freq_5, FUN = sort, decreasing = TRUE)
```

```{r form_tdms, echo=FALSE, cache=TRUE}
# Maker term document matrices for all 5 samples
tdm_1 <- TermDocumentMatrix(sample_text_1)
tdm_2 <- TermDocumentMatrix(sample_text_2)
tdm_3 <- TermDocumentMatrix(sample_text_3)
tdm_4 <- TermDocumentMatrix(sample_text_4)
tdm_5 <- TermDocumentMatrix(sample_text_5)
```

```{r freq_term_tables, echo=FALSE}
blog_monograms_table <- cbind(1:10, rownames(freq_1[[1]][1:10]), freq_1[[1]][1:10], rownames(freq_2[[1]][1:10]), freq_2[[1]][1:10], rownames(freq_3[[1]][1:10]), freq_3[[1]][1:10], rownames(freq_4[[1]][1:10]), freq_4[[1]][1:10], rownames(freq_5[[1]][1:10]), freq_5[[1]][1:10])

news_monograms_table <- cbind(1:10, rownames(freq_1[[2]][1:10]), freq_1[[2]][1:10], rownames(freq_2[[2]][1:10]), freq_2[[2]][1:10], rownames(freq_3[[2]][1:10]), freq_3[[2]][1:10], rownames(freq_4[[2]][1:10]), freq_4[[2]][1:10], rownames(freq_5[[2]][1:10]), freq_5[[2]][1:10])

twit_monograms_table <- cbind(1:10, rownames(freq_1[[3]][1:10]), freq_1[[3]][1:10], rownames(freq_2[[3]][1:10]), freq_2[[3]][1:10], rownames(freq_3[[3]][1:10]), freq_3[[3]][1:10], rownames(freq_4[[3]][1:10]), freq_4[[3]][1:10], rownames(freq_5[[3]][1:10]), freq_5[[3]][1:10])

rownames(blog_monograms_table) <- rep(x = "", times = 10)
blog_monograms_table

rownames(news_monograms_table) <- rep(x = "", times = 10)
news_monograms_table

rownames(twit_monograms_table) <- rep(x = "", times = 10)
twit_monograms_table

```

```{r, echo=FALSE, eval=FALSE}
#Load sample files
docs <- DirSource(directory = paste0(getwd(), "/samples/clean"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, removeNumbers)

texts <- tm_map(texts, removePunctuation)

texts <- tm_map(texts, removeWords, stopwords("english"))

#texts <- tm_map(texts, stemDocument)

cdf <- function(n){
        output <- numeric(n)
        output[1] <- freq[1]
        for(i in 2:n){
                output[i] <- freq[i] + output[i - 1]
        }
        
        100*output/numwords}
```

```{r, echo=FALSE, eval=FALSE}
# Create sample files
dir <- paste0(getwd(),"/final/en_US")
twit <- readLines(con = paste0(dir, "/en_US.twitter.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/en_US.blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/en_US.news.txt"), encoding = "UTF-8")

set.seed(3172016)

writeLines(text = twit[as.logical(rbinom(n = length(twit), size = 1, prob = .05))], con = paste0(getwd(), "/samples/twit.txt"))

writeLines(text = blogs[as.logical(rbinom(n = length(blogs), size = 1, prob = .05))], con = paste0(getwd(), "/samples/blogs.txt"))

writeLines(text = news[as.logical(rbinom(n = length(news), size = 1, prob = .05))], con = paste0(getwd(), "/samples/news.txt"))
```

```{r cleaning_data, echo=FALSE, eval=FALSE}
# I needed to remove the webadresses in preprocessing, because tm apparently doesn't like regex's.
# regex's are also a lot more elegant for eliminating swear words

dir <- paste0(getwd(),"/samples")
twit <- readLines(con = paste0(dir, "/twit.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/news.txt"), encoding = "UTF-8")

rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_web_addresses(twit)
blogs <- rm_web_addresses(blogs)
news <- rm_web_addresses(news)

# Remove multiword swears
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_complex_swear_words(twit)
blogs <- rm_complex_swear_words(blogs)
news <- rm_complex_swear_words(news)

writeLines(text = twit, con = paste0(getwd(), "/samples/clean/twit.txt"))

writeLines(text = blogs, con = paste0(getwd(), "/samples/clean/blogs.txt"))

writeLines(text = news, con = paste0(getwd(), "/samples/clean/news.txt"))
```

```{r, echo=FALSE, eval=FALSE}
#Assess 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

#freq <- termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=FALSE, eval=FALSE}
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

# termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=FALSE, eval=FALSE}
#Term-Document Matrix
tdm <- TermDocumentMatrix(texts)
```

```{r, echo=FALSE}
# The histogram of the number of characters in an n-gram forms a bell-shaped curve. Looking at the longstrings, these are mostly strings where the user failed to include spaces between words. So we don't want to train our algorithm on them.
```
