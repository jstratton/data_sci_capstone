---
title: "Natural Language Processing Exploratory Data Analysis"
author: "J. Stratton"
date: "March 16, 2016"
output: html_document
---

```{r load_libs, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(SnowballC)
```

# Introduction  

Natural Language Processing (NLP) is important because it eases the interaction between humans and computers. One problem in NLP is predicting the next word based on prior text input. Such technology is useful because it could potentially significantly reduce how much typing people need to do. The main problem is creating a model that is accurate enough to be helpful.

An important task in constructing a linguistic model is analyzing a collection of sample texts, called a corpus. I carried out some data cleaning and exploratory data analysis on a corpus consisting of blog posts, news articles, and twitter posts. Determining basic statistics for the corpus and assessing the feasibility of constructing a representative sample were the major goals of my analysis.

# Results  

```{r exploring_txts, echo=FALSE, cache=TRUE}
# Load full files
docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

```

```{r analyze_raw_txt, echo=FALSE, cache=TRUE}

line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

chars_per_line <- lapply(X = texts, FUN = function(x){nchar(x[[1]])})

words_per_line <- lapply(X = texts, FUN = function(x){
        sapply(X = strsplit(x[[1]], split = "[[:space:]]+"), FUN = length)
})
```

The first step is to determine the sizes of each document. Each line in a document is the number of distinct text entries within that document. Twitter posts are the largest category because they are short and don't take up a lot of memory.

Table 1: Document Line Counts

```{r output_raw_counts, echo=FALSE}
line_count_output <- data.frame(line_count, row.names = c("Blogs", "News", "Twitter"))
colnames(line_count_output) <- "Number of Lines"
line_count_output

words_per_line_output <- sapply(X = words_per_line, FUN = summary)

colnames(words_per_line_output) <- c("Blogs", "News", "Twitter")
```

Word count is another useful way to categorize the data. Blog posts were typically longer than news articles or twitter posts. Strangely, some of the lines of text had 0 words. This text was probably thrown out due to corrupted data.

Table 2: Document Word Count Data

```{r output_words_per_line, echo=FALSE}
words_per_line_output
```

```{r create_clean_corpuses, echo=FALSE, cache=TRUE}
# Clean the text in the corpus

docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

# Remove whitespace
texts <- tm_map(texts, stripWhitespace)

# Remove the rarely used low info characters from the corpus
texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

# Remove swear words part II
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_complex_swear_words))

# Remove webaddresses
rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_web_addresses))

# Remove stop words
texts <- tm_map(texts, removeWords, stopwords("english"))

# Remove any whitespace my cleaning regexes created
texts <- tm_map(texts, stripWhitespace)
```

```{r create_random_samples, echo=FALSE, cache=TRUE}
line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

# Use a random 5% sample from the corpus for each sample
pcnt <- 5
set.seed(3202016)

# Make sample 1
sample_text_1 <- texts

sample_text_1[[1]][[1]] <- sample(x = sample_text_1[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))
sample_text_1[[2]][[1]] <- sample(x = sample_text_1[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))
sample_text_1[[3]][[1]] <- sample(x = sample_text_1[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 2
sample_text_2 <- texts

sample_text_2[[1]][[1]] <- sample(x = sample_text_2[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_2[[2]][[1]] <- sample(x = sample_text_2[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_2[[3]][[1]] <- sample(x = sample_text_2[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 3
sample_text_3 <- texts

sample_text_3[[1]][[1]] <- sample(x = sample_text_3[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_3[[2]][[1]] <- sample(x = sample_text_3[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_3[[3]][[1]] <- sample(x = sample_text_3[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 4
sample_text_4 <- texts

sample_text_4[[1]][[1]] <- sample(x = sample_text_4[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_4[[2]][[1]] <- sample(x = sample_text_4[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_4[[3]][[1]] <- sample(x = sample_text_4[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 5
sample_text_5 <- texts

sample_text_5[[1]][[1]] <- sample(x = sample_text_5[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_5[[2]][[1]] <- sample(x = sample_text_5[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_5[[3]][[1]] <- sample(x = sample_text_5[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

```

```{r analyze_rand_samples, echo=FALSE, cache=TRUE}
# Analyze 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Analyze 2- through 4-grams
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)


n_gram_freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
n_gram_freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
n_gram_freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
n_gram_freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
n_gram_freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Sort the frequency lists
freq_1 <- lapply(X = freq_1, FUN = sort, decreasing = TRUE)
freq_2 <- lapply(X = freq_2, FUN = sort, decreasing = TRUE)
freq_3 <- lapply(X = freq_3, FUN = sort, decreasing = TRUE)
freq_4 <- lapply(X = freq_4, FUN = sort, decreasing = TRUE)
freq_5 <- lapply(X = freq_5, FUN = sort, decreasing = TRUE)

n_gram_freq_1 <- lapply(X = n_gram_freq_1, FUN = sort, decreasing = TRUE)
n_gram_freq_2 <- lapply(X = n_gram_freq_2, FUN = sort, decreasing = TRUE)
n_gram_freq_3 <- lapply(X = n_gram_freq_3, FUN = sort, decreasing = TRUE)
n_gram_freq_4 <- lapply(X = n_gram_freq_4, FUN = sort, decreasing = TRUE)
n_gram_freq_5 <- lapply(X = n_gram_freq_5, FUN = sort, decreasing = TRUE)
```

```{r form_tdms, echo=FALSE, cache=TRUE}
# Maker term document matrices for all 5 samples
tdm_1 <- TermDocumentMatrix(sample_text_1)
tdm_2 <- TermDocumentMatrix(sample_text_2)
tdm_3 <- TermDocumentMatrix(sample_text_3)
tdm_4 <- TermDocumentMatrix(sample_text_4)
tdm_5 <- TermDocumentMatrix(sample_text_5)
```

Next, I found the frequencies of different words in each document. I tabulated the top ten most common words by medium below. All of the words on the list are common English words. The only suprise is that numeric phrases like "one" or "two" appeared on this list. Note that the number of counts is radically different between mediums.

Table 3: Top Ten Most Common Words by Medium

```{r freq_term_tables, echo=FALSE}
all_monograms_table <- cbind(1:10, rownames(freq_1[[1]][1:10]), freq_1[[1]][1:10], rownames(freq_1[[2]][1:10]), freq_1[[2]][1:10], rownames(freq_1[[3]][1:10]), freq_1[[3]][1:10])

rownames(all_monograms_table) <- rep(x = "", times = 10)
colnames(all_monograms_table) <- c("Rank", "Blogs", "Counts", "News", "Counts", "Twitter", "Counts")

all_monograms_table
```

Below I tabulated the most common phrases by medium. All of the phrases sound like common English phrases. This means that the sample size is large enough to be unbiased after data cleaning. Having these common English phrases will be helpful for training our lignuistic model. Major cities appearing on the Blogs and News list will be helpful for teaching my machine learning algorithm to recognize those locations.

Table 4: Top Ten Most Common Phrases by Medium

```{r n_gram_tables, echo=FALSE}
all_ngrams_table <- cbind(1:10, rownames(n_gram_freq_1[[1]][1:10]), n_gram_freq_1[[1]][1:10], rownames(n_gram_freq_1[[2]][1:10]), n_gram_freq_1[[2]][1:10], rownames(n_gram_freq_1[[3]][1:10]), n_gram_freq_1[[3]][1:10])

rownames(all_ngrams_table) <- rep(x = "", times = 10)
colnames(all_ngrams_table) <- c("Rank", "Blogs", "Counts", "News", "Counts", "Twitter", "Counts")

all_ngrams_table
```

A useful plot for exploring the data is to plot frequency vs. word rank. All three plots have a large peak near the low ranked words and quickly tail off. Such patterns are common in linguistic data. This is a good sign that the sample size is large enough to be representative of the data.

```{r freq_dist_plots, echo=FALSE}
# Make a 1x3 plot of the frequencies
par(mfcol = c(1,3))

plot(x = 1:length(freq_1[[1]]), y = freq_1[[1]], type = "l", main = "Blog Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
plot(x = 1:length(freq_1[[2]]), y = freq_1[[2]], type = "l", main = "News Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
plot(x = 1:length(freq_1[[3]]), y = freq_1[[3]], type = "l", main = "Twitter Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
```

Figure 1: Word Frequency vs. Word Rank(in descending order).

We can make similar plots for the corpus as a whole. According to Zipf's Law, our word frequency distribution should be a straight line on a log-log scale. Our frequency curve is nearly a straight line for the high ranked words (i.e. uncommon ones). But it doesn't follow the straight line for low ranked words. This Zipf plot was constructed from an aggregate of all three document types. Bias from different total word counts for each medium is the most likely cause for this discrepancy. 

```{r diagnostic_plots, echo=FALSE}
# Make a 1x3 plot of the frequencies
par(mfcol = c(1,2))

Zipf_1 <- Zipf_plot(x = tdm_1, lwd = 5, lty = 3, col = "blue", main = "Zipf Plot")
Heaps_1 <- Heaps_plot(x = tdm_1, lwd = 5, lty = 3, col = "blue", main = "Heaps Plot", xlab = "log(total no. of words)", ylab = "log(vocabulary size)")
```

Figure 2: Zipf Plot (Right): Word frequency is plotted against word rank on a log-log scale. The word frequency plot (dashed blue line) would follow the best fit line (black line) if the data followed Zipf's Law perfectly. Heaps Plot (Left): Vocabular size is plotted as a function of number of words on a log-log scale (dashed blue line). This perfectly matches a linear fit on a log-log scale (black line). Note that these are natural logs, and not base-10 logs.

However, our data set matches Heaps Law quite well. In other words, there will be a law of diminishing returns for increasing the vocabulary for our mode. Based on the total wordcount, I would extrapolate a vocabulary size of `r  format(ceiling(exp((Heaps_1[2]*log(sum(sapply(X = words_per_line, FUN = sum))) + Heaps_1[1]))), scientific = FALSE)` for this corpus.

# Conclusion

Our corpus appears to follow normal linguistic patterns. High prevalence of common English phrases will make it possible to create a model for predicting the next word in a phrase. Two word phrases look particularly interesting for training our model. However, my data cleaning algorithm created empty rows in the data set. This is worrying considering that these empty sets occured in some of the more verbose document types. I will need to determine if these empty rows are being thrown out correctly.

During my exploratory data analysis, I took 5 random text samples from the corpus to determine the consistency of the results. I found that a sample size of 5% yields consistent results across samples in terms of the most common words. All 5 samples also had nearly identical Zipf and Heaps plots. I plan to use a 5% sample of the data to train my algorithm going forward.

Going forward, I will need to research models for predicting words. Considering how complex language is, trees will probably be too inefficient for our purposes. A generalized linear model or similar will probably be the best approach. Stemming the words in the Corpus down to their root form may help to reduce the size of the dictionary required to model the linguistics correctly. Completing the stem when determining the predicted word will be the challenge here.

Predicting capitalization will be particularlly challenging. Hard coding capitalization based on punctuation will probably be more memory efficient than using machine learning for that purpose. On the other hand, teaching the model to capitalize proper names correctly will probably require machine learning. One approach to this is to determine the ratio of capitalization for a word. If it is over 50%, then the model can assume that the word will be capitalized. Or I could keep the capitalization in the Corpus throughout the process and use that to train my model. I will need to look into this problem further.

# Appendix: R Code used to Generate this Report

## Load Required Libraries  

```{r, echo=TRUE, eval=FALSE}
library(tm)
library(RWeka)
library(SnowballC)
```

## Load Raw Texts  

```{r, echo=TRUE, eval=FALSE}
# Load full files
docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

```

## Analyze Raw Texts

```{r, echo=TRUE, eval=FALSE}

line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

chars_per_line <- lapply(X = texts, FUN = function(x){nchar(x[[1]])})

words_per_line <- lapply(X = texts, FUN = function(x){
        sapply(X = strsplit(x[[1]], split = "[[:space:]]+"), FUN = length)
})
```

## Output the Results of the Raw Text Analysis

```{r, echo=TRUE, eval=FALSE}
line_count_output <- data.frame(line_count, row.names = c("Blogs", "News", "Twitter"))
colnames(line_count_output) <- "Number of Lines"
line_count_output

chars_per_line_output <- sapply(X = chars_per_line, FUN = summary)
words_per_line_output <- sapply(X = words_per_line, FUN = summary)

colnames(chars_per_line_output) <- c("Blogs", "News", "Twitter")
colnames(words_per_line_output) <- c("Blogs", "News", "Twitter")

chars_per_line_output
words_per_line_output
```

## Produce a Clean Copy of the Text

```{r, echo=TRUE, eval=FALSE}
# Clean the text in the corpus

docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

# Remove whitespace
texts <- tm_map(texts, stripWhitespace)

# Remove the rarely used low info characters from the corpus
texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

# Remove swear words part II
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_complex_swear_words))

# Remove webaddresses
rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_web_addresses))

# Remove stop words
texts <- tm_map(texts, removeWords, stopwords("english"))

# Remove any whitespace my cleaning regexes created
texts <- tm_map(texts, stripWhitespace)
```

## Take Random Samples from the Data Set

Used to check for bias. Results from Sample 1 were shown in order to minimize the length of this report. Similar results were found in Samples 2-5.

```{r, echo=TRUE, eval=FALSE}
line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

# Use a random 5% sample from the corpus for each sample
pcnt <- 5
set.seed(3202016)

# Make sample 1
sample_text_1 <- texts

sample_text_1[[1]][[1]] <- sample(x = sample_text_1[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))
sample_text_1[[2]][[1]] <- sample(x = sample_text_1[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))
sample_text_1[[3]][[1]] <- sample(x = sample_text_1[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 2
sample_text_2 <- texts

sample_text_2[[1]][[1]] <- sample(x = sample_text_2[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_2[[2]][[1]] <- sample(x = sample_text_2[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_2[[3]][[1]] <- sample(x = sample_text_2[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 3
sample_text_3 <- texts

sample_text_3[[1]][[1]] <- sample(x = sample_text_3[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_3[[2]][[1]] <- sample(x = sample_text_3[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_3[[3]][[1]] <- sample(x = sample_text_3[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 4
sample_text_4 <- texts

sample_text_4[[1]][[1]] <- sample(x = sample_text_4[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_4[[2]][[1]] <- sample(x = sample_text_4[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_4[[3]][[1]] <- sample(x = sample_text_4[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Make sample 5
sample_text_5 <- texts

sample_text_5[[1]][[1]] <- sample(x = sample_text_5[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))

sample_text_5[[2]][[1]] <- sample(x = sample_text_5[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))

sample_text_5[[3]][[1]] <- sample(x = sample_text_5[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

```
## Analyze the Random Text Samples  

```{r, echo=TRUE, eval=FALSE}
# Analyze 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Analyze 2- through 4-grams
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)


n_gram_freq_1 <- lapply(X = sample_text_1, FUN = termFreq, control = ctrl)
n_gram_freq_2 <- lapply(X = sample_text_2, FUN = termFreq, control = ctrl)
n_gram_freq_3 <- lapply(X = sample_text_3, FUN = termFreq, control = ctrl)
n_gram_freq_4 <- lapply(X = sample_text_4, FUN = termFreq, control = ctrl)
n_gram_freq_5 <- lapply(X = sample_text_5, FUN = termFreq, control = ctrl)

# Sort the frequency lists
freq_1 <- lapply(X = freq_1, FUN = sort, decreasing = TRUE)
freq_2 <- lapply(X = freq_2, FUN = sort, decreasing = TRUE)
freq_3 <- lapply(X = freq_3, FUN = sort, decreasing = TRUE)
freq_4 <- lapply(X = freq_4, FUN = sort, decreasing = TRUE)
freq_5 <- lapply(X = freq_5, FUN = sort, decreasing = TRUE)

n_gram_freq_1 <- lapply(X = n_gram_freq_1, FUN = sort, decreasing = TRUE)
n_gram_freq_2 <- lapply(X = n_gram_freq_2, FUN = sort, decreasing = TRUE)
n_gram_freq_3 <- lapply(X = n_gram_freq_3, FUN = sort, decreasing = TRUE)
n_gram_freq_4 <- lapply(X = n_gram_freq_4, FUN = sort, decreasing = TRUE)
n_gram_freq_5 <- lapply(X = n_gram_freq_5, FUN = sort, decreasing = TRUE)
```

## Create Term-Document-Matrices

```{r, echo=TRUE, eval=FALSE}
# Maker term document matrices for all 5 samples
tdm_1 <- TermDocumentMatrix(sample_text_1)
tdm_2 <- TermDocumentMatrix(sample_text_2)
tdm_3 <- TermDocumentMatrix(sample_text_3)
tdm_4 <- TermDocumentMatrix(sample_text_4)
tdm_5 <- TermDocumentMatrix(sample_text_5)
```

## Output for Displaying the 1-Gram Table

```{r, echo=TRUE, eval=FALSE}
all_monograms_table <- cbind(1:10, rownames(freq_1[[1]][1:10]), freq_1[[1]][1:10], rownames(freq_1[[2]][1:10]), freq_1[[2]][1:10], rownames(freq_1[[3]][1:10]), freq_1[[3]][1:10])

rownames(all_monograms_table) <- rep(x = "", times = 10)
colnames(all_monograms_table) <- c("Rank", "Blogs", "Counts", "News", "Counts", "Twitter", "Counts")

all_monograms_table
```
## Code for Producing the n-gram Table

```{r, echo=TRUE, eval=FALSE}
all_ngrams_table <- cbind(1:10, rownames(n_gram_freq_1[[1]][1:10]), n_gram_freq_1[[1]][1:10], rownames(n_gram_freq_1[[2]][1:10]), n_gram_freq_1[[2]][1:10], rownames(n_gram_freq_1[[3]][1:10]), n_gram_freq_1[[3]][1:10])

rownames(all_ngrams_table) <- rep(x = "", times = 10)
colnames(all_ngrams_table) <- c("Rank", "Blogs", "Counts", "News", "Counts", "Twitter", "Counts")

all_ngrams_table
```

## Make a Frequency-Rank Plot for all Three Document Sources

```{r, echo=TRUE, eval=FALSE}
# Make a 1x3 plot of the frequencies
par(mfcol = c(1,3))

plot(x = 1:length(freq_1[[1]]), y = freq_1[[1]], type = "l", main = "Blog Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
plot(x = 1:length(freq_1[[2]]), y = freq_1[[2]], type = "l", main = "News Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
plot(x = 1:length(freq_1[[3]]), y = freq_1[[3]], type = "l", main = "Twitter Word Frequencies", xlab = "Word Rank", ylab = "Word Frequency(Counts)", lwd = 5)
```

## Make a Zipf and a Heaps Plot to determine if the data follows common Linguistic Trends.

```{r, echo=TRUE, eval=FALSE}
# Make a 1x3 plot of the frequencies
par(mfcol = c(1,2))

Zipf_1 <- Zipf_plot(x = tdm_1, lwd = 5, lty = 3, col = "blue", main = "Zipf Plot")
Heaps_1 <- Heaps_plot(x = tdm_1, lwd = 5, lty = 3, col = "blue", main = "Heaps Plot", xlab = "log(total no. of words)", ylab = "log(vocabulary size)")
```