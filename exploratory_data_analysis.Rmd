---
title: "Exploratory Data Analysis"
author: "J. Stratton"
date: "March 16, 2016"
output: html_document
---

```{r, echo=TRUE}
library(tm)
library(RWeka)
library(SnowballC)
```

```{r, echo=TRUE}
#Load sample files
docs <- DirSource(directory = paste0(getwd(), "/samples"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, removeNumbers)

texts <- tm_map(texts, removePunctuation)

texts <- tm_map(texts, removeWords, stopwords("english"))

texts <- tm_map(texts, stemDocument)

cdf <- function(n){
        output <- numeric(n)
        output[1] <- freq[1]
        for(i in 2:n){
                output[i] <- freq[i] + output[i - 1]
        }
        
        100*output/numwords}
```

```{r, echo=TRUE}
# Create sample files
dir <- paste0(getwd(),"/final/en_US")
twit <- readLines(con = paste0(dir, "/en_US.twitter.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/en_US.blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/en_US.news.txt"), encoding = "UTF-8")

set.seed(3172016)

writeLines(text = twit[as.logical(rbinom(n = length(twit), size = 1, prob = .05))], con = paste0(getwd(), "/samples/twit.txt"))

writeLines(text = blogs[as.logical(rbinom(n = length(blogs), size = 1, prob = .05))], con = paste0(getwd(), "/samples/blogs.txt"))

writeLines(text = news[as.logical(rbinom(n = length(news), size = 1, prob = .05))], con = paste0(getwd(), "/samples/news.txt"))
```

```{r cleaning_data, echo=TRUE}
# I needed to remove the webadresses in preprocessing, because tm apparently doesn't like regex's.
# regex's are also a lot more elegant for eliminating swear words

dir <- paste0(getwd(),"/samples")
twit <- readLines(con = paste0(dir, "/twit.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/news.txt"), encoding = "UTF-8")

rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_web_addresses(twit)
blogs <- rm_web_addresses(blogs)
news <- rm_web_addresses(news)

# Remove multiword swears
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_complex_swear_words(twit)
blogs <- rm_complex_swear_words(blogs)
news <- rm_complex_swear_words(news)

writeLines(text = twit, con = paste0(getwd(), "/samples/clean/twit.txt"))

writeLines(text = blogs, con = paste0(getwd(), "/samples/clean/blogs.txt"))

writeLines(text = news, con = paste0(getwd(), "/samples/clean/news.txt"))
```

```{r, echo=TRUE}
#Assess 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

#freq <- termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=TRUE}
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

# termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=TRUE}
#Term-Document Matrix
tdm <- TermDocumentMatrix(texts)
```

```{r, echo=FALSE}
# The histogram of the number of characters in an n-gram forms a bell-shaped curve. Looking at the longstrings, these are mostly strings where the user failed to include spaces between words. So we don't want to train our algorithm on them.
```
