---
title: "Exploratory Data Analysis"
author: "J. Stratton"
date: "March 16, 2016"
output: pdf
---

```{r load_libs, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(SnowballC)
```

```{r exploring_txts, echo=FALSE}
# Load full files
docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

```

```{r analyze_raw_txt, echo=FALSE}

line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

chars_per_line <- lapply(X = texts, FUN = function(x){nchar(x[[1]])})

words_per_line <- lapply(X = texts, FUN = function(x){
        sapply(X = strsplit(x[[1]], split = "[[:space:]]+"), FUN = length)
})
```

```{r output_raw_counts, echo=FALSE}
line_count_output <- data.frame(line_count, row.names = c("Blogs", "News", "Twitter"))
colnames(line_count_output) <- "Number of Lines"
line_count_output

chars_per_line_output <- sapply(X = chars_per_line, FUN = summary)
words_per_line_output <- sapply(X = words_per_line, FUN = summary)

colnames(chars_per_line_output) <- c("Blogs", "News", "Twitter")
colnames(words_per_line_output) <- c("Blogs", "News", "Twitter")

chars_per_line_output
words_per_line_output
```

```{r create_clean_corpus, echo=FALSE}

docs <- DirSource(directory = paste0(getwd(),"/final/en_US"), encoding = "UTF-8")

texts <- VCorpus(docs)

line_count <- sapply(X = texts, FUN = function(x){length(x[[1]])})

# Use a random representative 5% sample from the corpus
pcnt <- 5
set.seed(3202016)

texts[[1]][[1]] <- sample(x = texts[[1]][[1]], size = ceiling(pcnt*line_count[1]/100))
texts[[2]][[1]] <- sample(x = texts[[2]][[1]], size = ceiling(pcnt*line_count[2]/100))
texts[[3]][[1]] <- sample(x = texts[[3]][[1]], size = ceiling(pcnt*line_count[3]/100))

# Remove whitespace
texts <- tm_map(texts, stripWhitespace)

# Remove the rarely used low info characters from the corpus
texts <- tm_map(texts, content_transformer(gsub), pattern = "[^ [[:alpha:]]", replacement = "")

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

# Remove swear words part II
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_complex_swear_words))

# Remove webaddresses
rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

texts <- tm_map(texts, content_transformer(rm_web_addresses))

# Remove stop words
texts <- tm_map(texts, removeWords, stopwords("english"))

# Remove any whitespace my cleaning regexes created
texts <- tm_map(texts, stripWhitespace)
```

```{r, echo=FALSE}
#Load sample files
docs <- DirSource(directory = paste0(getwd(), "/samples/clean"), encoding = "UTF-8")

texts <- VCorpus(docs)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, content_transformer(tolower))

# Remove swear words
swears <- c("bastard", "bitch", "cunt", "damn", "fuck", "hell", "shit", "ass", "shitass")

texts <- tm_map(texts, removeWords, swears)

texts <- tm_map(texts, stripWhitespace)

texts <- tm_map(texts, removeNumbers)

texts <- tm_map(texts, removePunctuation)

texts <- tm_map(texts, removeWords, stopwords("english"))

#texts <- tm_map(texts, stemDocument)

cdf <- function(n){
        output <- numeric(n)
        output[1] <- freq[1]
        for(i in 2:n){
                output[i] <- freq[i] + output[i - 1]
        }
        
        100*output/numwords}
```

```{r, echo=TRUE}
# Create sample files
dir <- paste0(getwd(),"/final/en_US")
twit <- readLines(con = paste0(dir, "/en_US.twitter.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/en_US.blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/en_US.news.txt"), encoding = "UTF-8")

set.seed(3172016)

writeLines(text = twit[as.logical(rbinom(n = length(twit), size = 1, prob = .05))], con = paste0(getwd(), "/samples/twit.txt"))

writeLines(text = blogs[as.logical(rbinom(n = length(blogs), size = 1, prob = .05))], con = paste0(getwd(), "/samples/blogs.txt"))

writeLines(text = news[as.logical(rbinom(n = length(news), size = 1, prob = .05))], con = paste0(getwd(), "/samples/news.txt"))
```

```{r cleaning_data, echo=TRUE}
# I needed to remove the webadresses in preprocessing, because tm apparently doesn't like regex's.
# regex's are also a lot more elegant for eliminating swear words

dir <- paste0(getwd(),"/samples")
twit <- readLines(con = paste0(dir, "/twit.txt"), encoding = "UTF-8")
blogs <- readLines(con = paste0(dir, "/blogs.txt"), encoding = "UTF-8")
news <- readLines(con = paste0(dir, "/news.txt"), encoding = "UTF-8")

rm_web_addresses <- function(x){
        x <- gsub(pattern = "(http|w{3}).*?(com|edu|org| )", replacement = "", x = x, ignore.case = TRUE)
        
        # Get webadresses that appear at the end of a line.
        x <- gsub(pattern = "((http|w{3}).*?(com|edu|org|)*)$", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_web_addresses(twit)
blogs <- rm_web_addresses(blogs)
news <- rm_web_addresses(news)

# Remove multiword swears
rm_complex_swear_words <- function(x){
        # Swear words taken from https://en.wiktionary.org/wiki/Category:English_swear_words
        
        # Ass can't be removed easily because too many legitimate words have this word but asshole can be removed
        x <- gsub(pattern = "ass[:space:]*hole", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "god.*?damn", replacement = "", x = x, ignore.case = TRUE)
        
        #Taking out the word shit will get rid of "holy shit"
        
        x <- gsub(pattern = "son of a whore", replacement = "", x = x, ignore.case = TRUE)
        
        x <- gsub(pattern = "mother.*?fucker", replacement = "", x = x, ignore.case = TRUE)
        
        x
}

twit <- rm_complex_swear_words(twit)
blogs <- rm_complex_swear_words(blogs)
news <- rm_complex_swear_words(news)

writeLines(text = twit, con = paste0(getwd(), "/samples/clean/twit.txt"))

writeLines(text = blogs, con = paste0(getwd(), "/samples/clean/blogs.txt"))

writeLines(text = news, con = paste0(getwd(), "/samples/clean/news.txt"))
```

```{r, echo=TRUE}
#Assess 1-grams
strsplit_space_tokenizer <- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))

ctrl <- list(tokenize = strsplit_space_tokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

#freq <- termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=TRUE}
bigramtokenizer <- function(x){NGramTokenizer(x, Weka_control(min = 2, max = 4))}

ctrl <- list(tokenize = bigramtokenizer, tolower = FALSE)

freq <- lapply(X = texts, FUN = termFreq, control = ctrl)

# termFreq(doc = texts[[3]], control = ctrl)

# freq <- sort(x = freq, decreasing = TRUE)

freq <- lapply(X = freq, FUN = sort, decreasing = TRUE)

#numwords <- sum(freq)

#plot(x = 1:10, y = cdf(10))
```

```{r, echo=TRUE}
#Term-Document Matrix
tdm <- TermDocumentMatrix(texts)
```

```{r, echo=FALSE}
# The histogram of the number of characters in an n-gram forms a bell-shaped curve. Looking at the longstrings, these are mostly strings where the user failed to include spaces between words. So we don't want to train our algorithm on them.
```
